# 组会PPT讲稿

#### PPT1

发表：17年四月，world wide web会议，深度学习的网络结构，训练方法，GPU硬件的不断进步，促使其不断征服其他领域

何向南：中科大教授，92年，28岁

#### PPT2

点积和矩阵分解的关系：矩阵分解为两个矩阵相乘，又等价于第i行和第j列的点积

矩阵分解的限制性：Jaccard系数作为实际的结果，先计算u1，2，3，而后添加u4，发现，4和2的距离一定比4和3的距离更近

题外话：

 Jaccard 主要用于判断集合间相似度，所以他无法像矩阵一样，体现更多的信息。

Cosine 的计算中，则可以把用户对电影的评分信息加进去。

#### PPT3:

目标：NCF,GMF,MLP,NeuMF

ranking loss：度量学习，相对位置，**the objective of Ranking Losses is to predict relative distances between inputs**. This task if often called **metric learning**.

解决方式：使用大量的隐藏因子去学习交互函数。

#### PPT4:

$$
y_{ui}= \begin{cases}  1,\ if\ interaction\ is\ observed;\\0,\ else
\end{cases}
$$



#### PPT5:

element-wise product:按元素积

将GMF作为一种特殊的NCF

如果a是恒等函数，h是1的均匀向量

#### PPT6:

经验：tower structure，halving the layer size for each successive higher layer

#### PPT7:

generalization ability：泛化能力，适应新样本的能力

#### PPT8:

神经张量网络，使用加法

#### PPT9:

神经矩阵分解，使用连结操作

#### PPT10:

显示评分：回归损失，预测一个值，**平方损失**

隐式交互：分类损失，预测离散结果，logistic 

优化方法：随机梯度下降法

#### PPT11:

实验环境设置：

数据集，留一法，top-k排序，

HR@10：分母是所有的测试集合，分子式每个用户top-K推荐列表中属于测试集合的个数的总和

NDCG@10：最终所产生的增益（归一化折损累计增益）

#### PPT12:

BPR:基于矩阵分解的一种排序算法，针对每一个用户自己的商品喜好分贝做排序优化。在实际产品中，BPR之类的推荐排序在海量数据中选择极少量数据做推荐的时候有优势。淘宝京东有在用。部分填充，速度十分快。

eALS:最新的关于隐式数据的协同过滤算法，用一步到位的计算公式全部填充缺失值。

#### PPT13,14:

rel表示关联性，就是跟所想要的结果的关联度，0表示没有关联，越高说明关联性越高

i是位置，关联性乘以位置，就是第i个结果所产生的效益

IDCG是理想化的最大效益。

#### PPT15:

NeuMF，%5更优

NeuMF > GMF > MLP 

#### PPT16:

理论成果

#### PPT18:

GMF: weights w can simply be absorbed into the embeddings matrices P and Q

总之，应用场景（数据集）不同，采用的方法应该不同，灵活使用推荐算法或模型