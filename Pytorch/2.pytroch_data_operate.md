## 2、预备知识

### 2.2数据操作

#### 2.2.1创建tensor的函数

```python
torch.empty(5, 3)

torch.rand(5, 3)

torch.zeros(5, 3, dtype=torch.long)

torch.tensor([5.5, 3]) # 根据数组创建

# 获取形状

print(x.size())

print(x.shape)
```

还有很多函数可以创建`Tensor`，去翻翻官方API就知道了，下表给了一些常用的作参考。

|               函数                |           功能            |
| :-------------------------------: | :-----------------------: |
|          Tensor(*sizes)           |       基础构造函数        |
|           tensor(data,)           |  类似np.array的构造函数   |
|           ones(*sizes)            |         全1Tensor         |
|           zeros(*sizes)           |         全0Tensor         |
|            eye(*sizes)            |    对角线为1，其他为0     |
|         arange(s,e,step)          |    从s到e，步长为step     |
|        linspace(s,e,steps)        | 从s到e，均匀切分成steps份 |
|        rand/randn(*sizes)         |       均匀/标准分布       |
| normal(mean,std)/uniform(from,to) |     正态分布/均匀分布     |
|            randperm(m)            |         随机排列          |

这些创建方法都可以在创建的时候指定数据类型dtype和存放device(cpu/gpu)。





#### 2.2.2操作

三种加法：

```python
x + y

torch.add(x, y)

y.add_(x)  # 就地加法，类似x.copy_(y), x.t_()
```



改变形状：

```python
# 不同的size，但是是共享data
y = x.view(15)
z = x.view(-1, 5)  # -1所指的维度可以根据其他维度的值推出来
print(x.size(), y.size(), z.size())
# torch.Size([5, 3]) torch.Size([15]) torch.Size([3, 5])

# 不同size，不同data的新副本
x_cp = x.clone().view(15) # clone会被记录在计算图中，即梯度传播影响源数据

# reshape() 不保证返回的一定是拷贝后的数据
```



转换python数据

```python
x = torch.randn(1)
print(x)
print(x.item())
# tensor([2.3466])
# 2.3466382026672363
```



线性代数

另外，PyTorch还支持一些线性函数，这里提一下，免得用起来的时候自己造轮子，具体用法参考官方文档。如下表所示：

|               函数                |               功能                |
| :-------------------------------: | :-------------------------------: |
|               trace               |     对角线元素之和(矩阵的迹)      |
|               diag                |            对角线元素             |
|             triu/tril             | 矩阵的上三角/下三角，可指定偏移量 |
|              mm/bmm               |     矩阵乘法，batch的矩阵乘法     |
| addmm/addbmm/addmv/addr/baddbmm.. |             矩阵运算              |
|                 t                 |               转置                |
|             dot/cross             |             内积/外积             |
|              inverse              |             求逆矩阵              |
|                svd                |            奇异值分解             |

PyTorch中的`Tensor`支持超过一百种操作，包括转置、索引、切片、数学运算、线性代数、随机数等等，可参考[官方文档](https://pytorch.org/docs/stable/tensors.html)。



#### 2.2.3 广播机制

当对两个形状不同的`Tensor`按元素运算时，可能会触发广播（broadcasting）机制：先适当复制元素使这两个`Tensor`形状相同后再按元素运算。



#### 2.2.4 内存开销

```python
y[:] = y + x # 就地
y = y + x # 非就地
# view只是共享了tensor的数据，二者id（内存地址）并不一致。
# 因为tensor里面还有除了data的数据
```



#### 2.2.5 `Tensor`和NumPy相互转换

```python
# tensor to numpy
a = torch.ones(5)
b = a.numpy()
```

```python
# numpy to tensor
import numpy as np
a = np.ones(5)
b = torch.from_numpy(a)

# torch.tensor()将NumPy数组转换成Tensor
# 该方法总是会进行数据拷贝
c = torch.tensor(a)
```

所有在CPU上的`Tensor`（除了`CharTensor`）都支持与NumPy数组相互转换。



### 2.3自动求梯度

# 2.3 自动求梯度
在深度学习中，我们经常需要对函数求梯度（gradient）。PyTorch提供的[autograd](https://pytorch.org/docs/stable/autograd.html)包能够根据输入和前向传播过程自动构建计算图，并执行反向传播。



`Tensor`是这个包的核心类，如果将其属性`.requires_grad`设置为`True`，它将开始追踪(track)在其上的所有操作（这样就可以利用链式法则进行梯度传播了）。完成计算后，可以调用`.backward()`来完成所有梯度计算。此`Tensor`的梯度将累积到`.grad`属性中。



> 注意在`y.backward()`时，如果`y`是标量，则不需要为`backward()`传入任何参数；否则，需要传入一个与`y`同形的`Tensor`。解释见 2.3.2 节。
>
> 

如果不想要被继续追踪，可以调用`.detach()`将其从追踪记录中分离出来，这样就可以防止将来的计算被追踪，这样梯度就传不过去了。



可以用`with torch.no_grad()`将不想被追踪的操作代码块包裹起来，这种方法在评估模型的时候很常用，因为在评估模型时，我们并不需要计算可训练参数（`requires_grad=True`）的梯度。



`Function`是另外一个很重要的类。`Tensor`和`Function`互相结合就可以构建一个记录有整个计算过程的有向无环图（DAG）。



每个`Tensor`都有一个`.grad_fn`属性，该属性即创建该`Tensor`的`Function`, 就是说该`Tensor`是不是通过某些运算得到的，若是，则`grad_fn`返回一个与这些运算相关的对象，否则是None。



tensor.requires_grad 用于说明当前量是否需要在计算中保留对应的梯度信息



标量out，直接调用out.backward();不允许使用张量对张量求导，需要加权平均，例如out.backward(w);



如果我们想要修改`tensor`的数值，但是又不希望被`autograd`记录（即不会影响反向传播），那么我么可以对`tensor.data`进行操作。