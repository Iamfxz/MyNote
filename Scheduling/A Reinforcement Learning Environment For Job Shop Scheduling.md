# 《A Reinforcement Learning Environment For Job-Shop Scheduling》

### 问题：

1. JSSP的输入和输出方式？
2. JSSP的处理过程？
3. 为什么makespan结果会比其他的论文好？
4. 代码中有哪些细节值得借鉴？
5. 论文中的其他困惑点？



### 相关论文：

作者使用单智能体和多智能体来划分目前DRL方法的两大类

1. 多智能体：2018 DQN，2020 DDPG效果更好
2. 单智能体：2020 GNN+MLP，2020 CNN(处理时间，时间表，机器利用率)+DRL，2019 DQN+Rule



### IO过程(问题1回答)：

1. 一次只读入一个实例
2. 使用标准的JSSP文件（txt）方式读写，样例可看困惑点2
3. 一个二维数组solution，若是-1表示没排，否则为开始运行的时间步



### 处理过程(问题2回答)：

1. 离散集合={Job是否加工-J$_i$，空闲状态}，机器选择行为，行为对应作业和空闲
   1. 因此，对于每个Job而言，需要完成|M|个行为(机器的数量)。每个行为表示Job在每台机器上的加工时间，如果不需要在该机器上执行，可以当作一个0？
   2. 对于每个机器而言，需要运行至少|J|个与job有关的行为，加上若干个no-op（空闲）操作。
2. 每到一个时间步（可以定义为1秒或者1分），每台机器进行一个行为选择
3. 最终优先级：如果该Job已分配的行为数量等于|M|-1，说明最后一个动作已经确定了要在哪里执行。
4. 奖励函数：作者定义了一个schedule area，在这区间内计算reward，可以获得更多个奖励函数，从而为强化学习使用。



### 奖励函数细读(问题3回答):

$$
R(s, a)=p_{a j}-\sum_{m \in \mathcal{M}} \operatorname{empty}_{m}\left(s, s^{\prime}\right) \\
p_{aj}是作到某个时刻的已排程时间总和\\
a表示当前行为,s表示当前状态,s^{\prime}表示下一个状态 \\
emtpty_{m}(s, s^{\prime})是计算机器m在两个状态之间的idle时间\\
在实现的代码中，作者将所有的R都除以一个最大的工序加工时间来达到缩放
$$

该函数相对于makespan能得到更紧密的解。

1. 不用执行到最后一步，就能获得一个局部reward。
2. makespan可以看成是全局p+empty，而这里是局部的p-empty，那么改变之处在于系数-1和全局变为局部。



### 代码中的优点(问题4回答)：

1. 作者新建自己环境的代码十分受用，让我对新建gym的环境有了一个详细的了解

2. wandb和ray两个库的使用具有一定的启发，但可能更适合较大的项目

   1. wandb能够使得训练日志自动上传网络并绘图
   2. ray能够快速使得模型快速落地，并变成分布式，相比较spark更简单，更便捷

3. 之后打算：

   1. 进一步看懂代码，模型代码很少，基本已看完，但环境的代码很复杂，还没看完。
   2. 先使用pytorch替换掉里面的ray的ppo模型，然后逐步调试，理解环境部分的代码

   

### 困惑点(未解决，问题5)：

1. 第四页中的问题对称性，讲述了一些JSS问题的对称性，通过破坏对称性，而降低搜索解空间的大小

   1. 同一时间步长中，操作具有对称性，也就是操作的执行顺序待定。怎么解决？以及为什么说直接赋予机器从小到大的索引会导致失去全局视图。
   2. 同一机器中，"运行"操作和"无运行"操作具有对称性，也就是op和no-op两者的执行顺序待定。解决方案：当no-op时，其他的操作临时设未不可执行。以及通过非最终状态优先调度的规则。

2. 似乎JSSP问题是每个Job在每台机器上都有执行一定时间，还是说我看到的刚好是特例？

```python
15 15 # 作业数，机器数
6 94 12 66  4 10  7 53  3 26  2 15 10 65 11 82  8 10 14 27  9 93 13 92  5 96  0 70  1 83  # 奇数是机器索引，偶数是在该机器上的加工时间
4 74  5 31  7 88 14 51 13 57  8 78 11  8  9  7  6 91 10 79  0 18  3 51 12 18  1 99  2 33 
1  4  8 82  9 40 12 86  6 50 11 54 13 21  5  6  0 54  2 68  7 82 10 20  4 39  3 35 14 68 
5 73  2 23  9 30  6 30 10 53  0 94 13 58  4 93  7 32 14 91 11 30  8 56 12 27  1 92  3  9 
7 78  8 23  6 21 10 60  4 36  9 29  2 95 14 99 12 79  5 76  1 93 13 42 11 52  0 42  3 96 
5 29  3 61 12 88 13 70 11 16  4 31 14 65  7 83  2 78  1 26 10 50  0 87  9 62  6 14  8 30 
12 18  3 75  7 20  8  4 14 91  6 68  1 19 11 54  4 85  5 73  2 43 10 24  0 37 13 87  9 66 
11 32  5 52  0  9  7 49 12 61 13 35 14 99  1 62  2  6  8 62  4  7  3 80  9  3  6 57 10  7 
10 85 11 30  6 96 14 91  0 13  1 87  2 82  5 83 12 78  4 56  8 85  7  8  9 66 13 88  3 15 
6  5 11 59  9 30  2 60  8 41  0 17 13 66  3 89 10 78  7 88  1 69 12 45 14 82  4  6  5 13 
4 90  7 27 13  1  0  8  5 91 12 80  6 89  8 49 14 32 10 28  3 90  1 93 11  6  9 35  2 73 
2 47 14 43  0 75 12  8  6 51 10  3  7 84  5 34  8 28  9 60 13 69  1 45  3 67 11 58  4 87 
5 65  8 62 10 97  2 20  3 31  6 33  9 33  0 77 13 50  4 80  1 48 11 90 12 75  7 96 14 44 
8 28 14 21  4 51 13 75  5 17  6 89  9 59  1 56 12 63  7 18 11 17 10 30  3 16  2  7  0 35 
10 57  8 16 12 42  6 34  4 37  1 26 13 68 14 73 11  5  0  8  7 12  3 87  2 83  9 20  5 97 
   
```

   





### 复现代码时候遇到的问题(已解决，记录)：

1. 'env': 'JSSEnv:jss-v1'，该环境已经创建好并上传gym库中，作者使用的ray库会自动生成一个包含该环境名字的本地日志。而windows文件不允许冒号，所以需要修改ray库的底层代码中的日志文件名字生成方式。



### 代码阅读：

大体分三块
1. 强化学习环境：已经注册成gym库的环境，各种强化学习包都可以用。
2. CP.py：是使用OR tools的求解器，进行求解的方法，能达到比较好的解。
3. mian.py：使用ray中的PPO算法，只修改了一个全连接层，然后定义了一些参数，就可以使用作者的环境来训练。

其他读后感主要写在代码注释中，目前均已跑通。

![image-20220103201512024](https://image-1252566752.cos.ap-hongkong.myqcloud.com/20220103202835.png)